\chapter{Summary Conclusions}
\label{ch:summary-conclusions}

\minitoc

This thesis has contributed novel modelling techniques for sequential data, with a focus on online portfolio selection. The contributions, set out in the three chapters comprising Part II, are unified under the title `Bayesian Online Learning for Portfolio Management', and split into three approximately-homogeneous contribution chapters. Chapter~\ref{ch:pa-extensions} discussed generalisations of and probabilistic inference in a particular class of online model known as `online passive-aggressive learning', in which a weight vector is passively maintained or aggressively updated from round to round depending on whether or not it correctly predicted the most recent example. Chapter~\ref{ch:oo} dealt with the topic of online convex optimisation in dynamic environments, and by incorporating insight from Chapter~\ref{ch:pa-extensions}, contributed two novel online and data-dependent adaptation methods for tuning the learning rate in online gradient descent, arguably the most popular algorithm in this context. Finally, in Chapter~\ref{ch:empirical-eval}, we demonstrated the superiority of our proposed methods over existing ones in online portfolio selection experiments involving real-world data from the equity, fixed income and real-estate markets.

This chapter seeks to conclude the thesis by drawing together summary results and conclusions of the contributions set out in Part II.


\section{Extending Passive-Aggressive Learning}

Chapter~\ref{ch:pa-extensions} formulated some generalisations and a Bayesian treatment of online passive-aggressive (PA) algorithms, which have been used in various fields including finance \citep{pamr, olmar}, recommendation systems \citep{blondel14} and natural language processing \citep{bayespa}. The generic PA update rule (as per Chapter~\ref{ch:ol}, Eq. \eqref{eq:generic-pa-update-rule}) is intuitive, straightforward to implement and universally applicable to online classification, regression and uniclass tasks. However, it suffers from some drawbacks, namely \textit{i}) it is only valid for the $\epsilon$-insensitive loss function, \textit{ii}) it does not provide any guidelines as to how to set the underlying hyperparameters, and \textit{iii}) it is a point estimate, thereby failing to capture model and prediction uncertainty.

To address the first of these issues, we derived a generalised passive-aggressive learning framework, by replacing the $\epsilon$-insensitive loss in the generic PA learning problem's constraint (i.e.\ the constraint in Eq. \eqref{eq:generic-pa-optpb}) with a general loss function. By approximating this general loss function with its first-order Taylor polynomial around the current weight vector, we are able to apply the same solution methods used to arrive at the PA weight updates. The algorithms so defined overcome the specialisation of the original PA framework to the $\epsilon$-insensitive loss while maintaining elegance and scalability. Moreover, the derivation is didactically useful in the sense that it provides inspiration and insight for other algorithms described later in the thesis, including the passive-aggressive convex optimisation algorithm~\ref{alg:paco}.

A Bayesian treatment of PA learning was also discussed, motivated by a desire to circumvent the remaining aforementioned shortcomings, i.e.\ points \textit{ii}) and \textit{iii}), in addition to the framework's inability to take into account how the data are distributed. Inspired by the pseudo-likelihood and data-augmentation ideas from \citep{polson&scott}, the mixture representation \eqref{eq:ilf-likelihood-mixture-rep} is the key ingredient, since it significantly reduces the complexity of inference by bringing Bayesian tools for Gaussian linear models to bear on PA algorithms. We then contributed a novel online scheme for variational posterior inference in an augmented variable space, that additionally offers the benefit of automatic hyperparameter tuning. To make robust predictions, the approximate posterior of model weights, rather than point estimates as in traditional PA methods, is used. Once the approximate posterior is obtained from a given data set, we can regard it as the model prior when dealing with the arrival of new data, which makes the model suitable for online scenarios. 


\section{Adaptive Gradient Methods for Dynamic Online Optimisation}

Online convex optimisation is a prevalent topic in machine learning and its application in dynamic environments has recently received considerable interest in the literature. Whilst the online gradient descent algorithm is appealing in this context due to its numerous advantages, adjusting its learning-rate parameter is an important unresolved problem which requires tuning in practice. Previous work has relied on theoretical learning-rate schedules, under the assumption that certain parameters of the loss function such as smoothness or strong convexity constants are known. However, in practice, such parameters are unknown and the burden of specification rests upon the practitioner.

We presented a novel method for dynamically updating the learning-rate parameter according to the gradients received so far, which we believe provides a significant contribution to the long-standing problem of tuning this parameter. Unlike the schemes previously used in the literature, ours is completely hyperparameter-free and can thus be readily deployed without any manual intervention. Moreover, its applicability extends beyond the field of online convex optimisation, in particular to neural-network training (see Section~\ref{sec:oo-further-work}).

The aforementioned method was derived in three steps. First, we determined in Section~\ref{sec:ogd-criterion} which optimisation problem online gradient descent implicitly solves. Second, we provided a maximum a posteriori interpretation thereof in Section~\ref{sec:probabilistic-ogd}, which revealed that the algorithm implicitly imposes an isotropic Gaussian prior over the weights whose precision parameter coincides with the inverse learning rate. Third, in Section~\ref{sec:learning-rate-inference}, we proceeded to integrate out this precision parameter so as to obtain the true weight posterior (i.e.\ the posterior conditioned on the data but not on the learning rate), whose optimisation yields hyperparameter-free online gradient descent updates.

Finally, we also discussed how to adapt the generalised passive-aggressive learning framework from Section~\ref{sec:gpa} to the dynamic bandit setting. While we were able to readily carry out this adaptation, it still required estimating the latent gradients as a preliminary step, which we covered in Section~\ref{sec:gradient-estimates}.

Taken together, the two approaches summarised above represent a significant contribution to the literature on online convex optimisation in dynamic environments.


\section{Application to Online Portfolio Selection}

This chapter culminates our research on online Bayesian techniques for portfolio management. It presents two novel families of algorithms for online portfolio selection, namely `online maximum reversion' and `adaptive online mean reversion', in Sections~\ref{sec:olmax} and~\ref{sec:ada-mr}, respectively. The proposed approaches are able to overcome the limitations of existing state-of-the-art online portfolio selection strategies, discussed in Section~\ref{sec:empirical-eval-intro}, and were shown to deliver superior performance on the equity data sets widely used in literature and a real-world cross-asset data set, respectively.

Online maximum reversion is built upon the assumption that price-relative vectors change slowly over time. This was corroborated for the stock-market data set in Section~\ref{sec:olmax-motivation}, and permits the use of the loss function at time $t$ (parameterised by the $t$th price-relative vector) as a surrogate for the unobserved loss at time $t+1$. With this in mind, we approximate the optimal portfolio $\mathbf{b}_{t+1}^*$ by the minimiser $\mathbf{b}_{t+1}$ of the current loss function, as detailed in Eq. \eqref{eq:single-olmax-criterion}. This leads to a hyperparameter-free single-period online reversion algorithm that does not suffer from data-snooping bias, unlike any of its competitors. We also explored a multiperiod variant to alleviate the data-snooping issue affecting the most powerful online mean-reversion strategy known to date, namely `online moving average reversion'.

In addition to the data-snooping bias, existing online portfolio selection techniques are fraught with selection bias as they are restricted to equity markets and, to fix this limitation, we devised adaptive online mean reversion strategies. To this end, we started from the passive-aggressive mean reversion and online moving average reversion algorithms, and expressed their respective portfolio updates in the form of online gradient descent updates with data-driven, but hyperparameter-dependent, learning-rate schedules (see Eq. \eqref{eq:pamr-portfolio} and \eqref{eq:olmar-portfolio}, respectively). To reduce this hyperparameter dependency, we then replaced these learning-rate schedules with their equivalent maximum posterior counterparts derived from Eq. \eqref{eq:mapgrad-learning-rate}. The resulting strategies are more amenable to asset classes beyond equities, as well as to reasonable transaction fees.